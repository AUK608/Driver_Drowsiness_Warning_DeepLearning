{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e88ba4fe-301a-42e9-965f-820c257ada0d",
   "metadata": {},
   "source": [
    "Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ec9924-27be-41ac-a5ee-be5a3545e873",
   "metadata": {},
   "outputs": [],
   "source": [
    "###pytorch packages\n",
    "import torch\n",
    "#from torchvision.models.detection import ssd300_vgg16\n",
    "import torchvision\n",
    "from torchvision import transforms as torchtrans  \n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "###imports for creating custom backbone\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "\n",
    "###helper libraries\n",
    "#from engine import train_one_epoch, evaluate\n",
    "#import utils\n",
    "import imutils\n",
    "#import transforms as T\n",
    "from torchvision import transforms as T\n",
    "\n",
    "###for mAP, IoU metrics & confusion matrix\n",
    "from torchmetrics.detection import MeanAveragePrecision, IntersectionOverUnion\n",
    "\n",
    "###to process coco format json\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "'''###for image augmentations\n",
    "from albumentations import Resize, Compose\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "from albumentations.augmentations.transforms import Normalize'''\n",
    "\n",
    "###onnx related importings\n",
    "import torch.onnx\n",
    "import onnx\n",
    "import onnxruntime\n",
    "\n",
    "###openvino related imports\n",
    "import openvino as ov\n",
    "from openvino.tools import mo\n",
    "\n",
    "###other packages\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image, ImageDraw\n",
    "from imutils.video import VideoStream\n",
    "from imutils.video import FPS\n",
    "import imutils\n",
    "import random\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "import glob as glob\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "###matplotlib for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "###for ignoring warnings\n",
    "'''import warnings\n",
    "warnings.filterwarnings('ignore')'''\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae3fa15-bbb4-4caf-abb7-5d8f133adc1e",
   "metadata": {},
   "source": [
    "Dataset Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6049310-a83c-48de-9020-f6693fd5b097",
   "metadata": {},
   "outputs": [],
   "source": [
    "###create your own dataset func to handle the images and its resp. annotations(COCO) for resizing and other operations\n",
    "class DDWDataset(Dataset):\n",
    "    ###init functions for initializing different parameters passed\n",
    "    def __init__(self, root, re_width, re_height, annotation, transforms=None):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        self.coco = COCO(annotation)\n",
    "        self.re_height = re_height\n",
    "        self.re_width = re_width\n",
    "        self.ids = list(sorted(self.coco.imgs.keys()))\n",
    "\n",
    "    ###function to process the data and access it\n",
    "    def __getitem__(self, index):\n",
    "        ###Own coco file\n",
    "        coco = self.coco\n",
    "        ###Image ID\n",
    "        img_id = self.ids[index]\n",
    "        ###List: get annotation id from coco\n",
    "        ann_ids = coco.getAnnIds(imgIds=img_id)\n",
    "        ###Dictionary: target coco_annotation file for an image\n",
    "        coco_annotation = coco.loadAnns(ann_ids)\n",
    "        ###get the class/category id --> [0:Background, 1:awake, 2:drowsy] \n",
    "        ann_cls = int(coco_annotation[0]['category_id'])\n",
    "        ###path for input image\n",
    "        path = os.path.join(self.root,coco.loadImgs(img_id)[0]['file_name'])\n",
    "        ###open the input image\n",
    "        img = cv2.imread(path)\n",
    "        ###convert BGR to RGB color format(because in cv2 by default its BGR) and represent to float32 as pytorch requires float values for training\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        ###resize all the image to single size\n",
    "        image_resized = cv2.resize(img, (self.re_width, self.re_height))\n",
    "        ###scale the raw pixel intensities to the range [0, 1]\n",
    "        image_resized /= 255.0\n",
    "\n",
    "        ###number of objects in the image\n",
    "        num_objs = len(coco_annotation)\n",
    "        ###get the height and width of the image to resize boxes/annotations\n",
    "        image_width = img.shape[1]\n",
    "        image_height = img.shape[0]\n",
    "\n",
    "        ###Bounding boxes for objects\n",
    "        ###coco format is bbox = [xmin, ymin, width, height]\n",
    "        ###In pytorch, the input should be [xmin, ymin, xmax, ymax]\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for i in range(num_objs):\n",
    "            labels.append(ann_cls)\n",
    "            \n",
    "            xmin = coco_annotation[i]['bbox'][0]\n",
    "            ymin = coco_annotation[i]['bbox'][1]\n",
    "            # In coco format, bbox = [xmin, ymin, width, height]\n",
    "            # In pytorch, the input should be [xmin, ymin, xmax, ymax]\n",
    "            # so add x1 with width and y1 with height\n",
    "            xmax = coco_annotation[i]['bbox'][0] + coco_annotation[i]['bbox'][2]\n",
    "            ymax = coco_annotation[i]['bbox'][1] + coco_annotation[i]['bbox'][3]\n",
    "\n",
    "            xmin_final = (xmin/image_width)*self.re_width\n",
    "            xmax_final = (xmax/image_width)*self.re_width\n",
    "            ymin_final = (ymin/image_height)*self.re_height\n",
    "            ymax_final = (ymax/image_height)*self.re_height\n",
    "            \n",
    "            boxes.append([xmin_final, ymin_final, xmax_final, ymax_final])\n",
    "\n",
    "        ###convert boxes to tensor\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        ###convert labels/classes/annotations to tensors\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        ###convert img_id to tensor\n",
    "        img_id = torch.tensor([img_id])\n",
    "        ###calculate the area after resizing and no need to convert to tensor as its tensorised while getting calculated from boxes(which is tensorised)\n",
    "        areas = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        ###convert iscrowd to tensor and only 1 class at a time in the image so crows=0\n",
    "        #iscrowd = torch.zeros((0,), dtype=torch.int64)   ###gives empty tenosr as [] which throws error for torchmetrics, coco going further\n",
    "        #iscrowd = torch.as_tensor(coco_annotation[0]['iscrowd'], dtype=torch.int64)   ###based on above issue don't create empty\n",
    "        ###commented the above iscrowd because for torchmetrics its throwing error and in this usecase, its not useful\n",
    "\n",
    "        ###Annotation is in dictionary format\n",
    "        ann_dict = {}\n",
    "        ann_dict[\"boxes\"] = boxes\n",
    "        ann_dict[\"labels\"] = labels\n",
    "        ann_dict[\"image_id\"] = img_id\n",
    "        ann_dict[\"area\"] = areas\n",
    "        #ann_dict[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        ###check if any transformations(like augmentations-rotate, flip, etc.) are applied\n",
    "        if self.transforms is not None:\n",
    "            img_re = self.transforms(image_resized)\n",
    "\n",
    "        return img_re, ann_dict\n",
    "\n",
    "    ###func to get the length of the dataset passed \n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416b1c7f-ced5-42dd-b596-1d9b4151e2e0",
   "metadata": {},
   "source": [
    "Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a01744-5d83-4c47-a248-3846c8e937cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "###In my case, just added ToTensor, can add different transformations like resize, translate, etc.\n",
    "#from albumentations.pytorch import ToTensorV2\n",
    "def get_transform():\n",
    "    custom_transforms = []\n",
    "    custom_transforms.append(torchvision.transforms.ToTensor())\n",
    "    #custom_transforms.append(torchvision.transforms.ToTensorV2(p=1.0))\n",
    "    return torchvision.transforms.Compose(custom_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6df78bf-8cfa-4b4b-8939-7c83a7006227",
   "metadata": {},
   "source": [
    "Utils & Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad0fd6f-04d6-47f9-8fdd-dba50b6e1a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "###this class keeps track of the training and validation loss values and helps to get the average for each epoch as well\n",
    "class Averager:\n",
    "    def __init__(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "        \n",
    "    def send(self, value):\n",
    "        self.current_total += value\n",
    "        self.iterations += 1\n",
    "    \n",
    "    @property\n",
    "    def value(self):\n",
    "        if self.iterations == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1.0 * self.current_total / self.iterations\n",
    "    \n",
    "    def reset(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "\n",
    "###To handle the data loading as different images may have different number of objects and to handle varying size tensors as well\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129afaf0-5666-4899-8278-0159d2066384",
   "metadata": {},
   "source": [
    "Training Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10eb645-b12a-4229-89f1-9bfd7640168d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4 ###increase / decrease according to GPU memeory\n",
    "RESIZE_TO = 416 ###resize the image for training and transforms based on the lowest size image if images are of different sizes\n",
    "NUM_EPOCHS = 50 ###number of epochs to train for, Note : give even num as to save last data for last epoch as well\n",
    "START_EPOCH = 0 ###to start from 0 or 1st epoch if scratch training, if continuing training, this var will be replaced with last saved ckpt num\n",
    "###to train on GPU if available\n",
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "###training images and COCO JSON Annotations files directory\n",
    "TRAIN_DIR = \"G:\\\\Projects\\\\ADAS\\\\DDW\\\\Data\\\\Usable_Data\\\\train\"\n",
    "TRAIN_ANN_DIR = \"G:\\\\Projects\\\\ADAS\\\\DDW\\\\Data\\\\Usable_Data\\\\train\\\\train_3_annotations.coco.json\"\n",
    "###validation images and COCO JSON Annotations files directory\n",
    "VALID_DIR = \"G:\\\\Projects\\\\ADAS\\\\DDW\\\\Data\\\\Usable_Data\\\\valid\"\n",
    "VALID_ANN_DIR = \"G:\\\\Projects\\\\ADAS\\\\DDW\\\\Data\\\\Usable_Data\\\\valid\\\\valid_3_annotations.coco.json\"\n",
    "###classes: 0 index is reserved for background, 1: awake, 2: drowsy\n",
    "CLASSES = ['background', 'awake', 'drowsy']\n",
    "NUM_CLASSES = 3\n",
    "\n",
    "###location to save model and plots\n",
    "OUT_DIR = \"G:\\\\Projects\\\\ADAS\\\\DDW\\\\Output\"\n",
    "###name to save the trained model with\n",
    "MODEL_NAME = 'frcnn_custombackbone_model'\n",
    "\n",
    "SAVE_PLOTS_EPOCH = 4 ###save loss plots after these many epochs, Note : give even num as to save last data for last epoch as well\n",
    "SAVE_MODEL_EPOCH = 4 ###save model after these many epochs, Note : give even num as to save last data for last epoch as well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cee4b97-09e1-4a20-ac5b-3a75be137a45",
   "metadata": {},
   "source": [
    "DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f198459e-d993-4ac7-a82a-fe9a60dba0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "###prepare the final datasets and data loaders\n",
    "train_dataset = DDWDataset(TRAIN_DIR, RESIZE_TO, RESIZE_TO, TRAIN_ANN_DIR, get_transform())\n",
    "valid_dataset = DDWDataset(VALID_DIR, RESIZE_TO, RESIZE_TO, VALID_ANN_DIR, get_transform())\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of validation samples: {len(valid_dataset)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b069a77c-6bc3-42a2-8974-53e161fdb297",
   "metadata": {},
   "source": [
    "FRCNN-Resnet50 Model Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ba82d0-4754-4202-a8d8-2a31c182255d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_frcnn_resnet50(num_classes):\n",
    "    \n",
    "    ###load Faster RCNN model a fresh as the classes what we have are new(i.e., without pretrained) and with only 1 box detection per image\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False, box_detections_per_img=1) #(pretrained=True)\n",
    "    \n",
    "    ###get the number of input features \n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    ###define a new head for the detector with required number of classes\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes) \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da8847e-0646-43fa-854a-2706b2f4da83",
   "metadata": {},
   "source": [
    "FRCNN-CustomBackbone Model Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26faf345-757a-4517-830d-f74a078d3c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "###to create a custom backbone for frcnn with resnet block\n",
    "###creates 2 Residual block of ResNet in each sequential block\n",
    "class ResidualBlock(nn.Module):    \n",
    "    def __init__(\n",
    "        self, in_channels, out_channels, use_1x1conv=True, strides=1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels,\n",
    "                               kernel_size=3, padding=1, stride=strides)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels,\n",
    "                               kernel_size=3, padding=1)\n",
    "        if use_1x1conv:\n",
    "            self.conv3 = nn.Conv2d(in_channels, out_channels,\n",
    "                                   kernel_size=1, stride=strides)\n",
    "        else:\n",
    "            self.conv3 = None\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        inputs = x\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        if self.conv3:\n",
    "            inputs = self.conv3(inputs)\n",
    "        x += inputs\n",
    "        return F.relu(x)\n",
    "\n",
    "###creating a resnet block\n",
    "def create_resnet_block(input_channels, output_channels, num_residuals,):\n",
    "        resnet_block = []\n",
    "        for i in range(num_residuals):\n",
    "            if i == 0:\n",
    "                resnet_block.append(ResidualBlock(input_channels, output_channels,\n",
    "                                    use_1x1conv=True, strides=2))\n",
    "            else:\n",
    "                resnet_block.append(ResidualBlock(output_channels, output_channels))\n",
    "        return resnet_block\n",
    "\n",
    "###creating a custom resnet class with totally 5 sequential blocks\n",
    "class CustomResNet(nn.Module):\n",
    "    def __init__(self, num_classes=3):   ###with 3 classes\n",
    "        super().__init__()\n",
    "        self.block1 = nn.Sequential(nn.Conv2d(3, 16, kernel_size=7, stride=2, padding=3),\n",
    "                        nn.BatchNorm2d(16), nn.ReLU(),\n",
    "                        nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "        self.block2 = nn.Sequential(*create_resnet_block(16, 32, 2))\n",
    "        self.block3 = nn.Sequential(*create_resnet_block(32, 64, 2))\n",
    "        self.block4 = nn.Sequential(*create_resnet_block(64, 128, 2))\n",
    "        self.block5 = nn.Sequential(*create_resnet_block(128, 256, 2))\n",
    "\n",
    "        self.linear = nn.Linear(256, num_classes)\n",
    "\n",
    "    ###a forward function with 5 blocks\n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.block5(x)\n",
    "        bs, _, _, _ = x.shape\n",
    "        x = F.adaptive_avg_pool2d(x, 1).reshape(bs, -1)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "###finally creating model class by combining other functions     \n",
    "def create_model_frcnn_custombackbone(num_classes):\n",
    "    custom_resnet = CustomResNet(num_classes=3)   ###with 3 classes\n",
    "    block1 = custom_resnet.block1\n",
    "    block2 = custom_resnet.block2\n",
    "    block3 = custom_resnet.block3\n",
    "    block4 = custom_resnet.block4\n",
    "    block5 = custom_resnet.block5\n",
    "\n",
    "    backbone = nn.Sequential(\n",
    "        block1, block2, block3, block4, block5 \n",
    "    )\n",
    "\n",
    "    backbone.out_channels = 256\n",
    "\n",
    "    ###Generate anchors using the RPN where 5x3 anchors are used i.e., anchors with 5 different sizes and 3 different aspect ratios.\n",
    "    anchor_generator = AnchorGenerator(\n",
    "        sizes=((32, 64, 128, 256, 512),),\n",
    "        aspect_ratios=((0.5, 1.0, 2.0),)\n",
    "    )\n",
    "\n",
    "    ###Feature maps to perform RoI cropping if backbone returns a Tensor, `featmap_names` is expected to be [0] where which feature maps to use can be chosen\n",
    "    roi_pooler = torchvision.ops.MultiScaleRoIAlign(\n",
    "        featmap_names=['0'],\n",
    "        output_size=7,\n",
    "        sampling_ratio=2\n",
    "    )\n",
    "\n",
    "    ###pass the custom backbone in FRCNN model\n",
    "    model = FasterRCNN(\n",
    "        backbone=backbone,\n",
    "        num_classes=num_classes,\n",
    "        rpn_anchor_generator=anchor_generator,\n",
    "        box_roi_pool=roi_pooler\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6faa7683-1a9d-4baa-9f09-c6e872acd025",
   "metadata": {},
   "source": [
    "Training and Validation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447ed095-9e35-4b7c-8ea0-ba39ef695d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###function for running training iterations\n",
    "def train(train_data_loader, model):\n",
    "    print('Training...')\n",
    "    global train_itr\n",
    "    global train_loss_list\n",
    "    \n",
    "    ###initialize tqdm progress bar\n",
    "    prog_bar = tqdm(train_data_loader, total=len(train_data_loader))\n",
    "    \n",
    "    for i, data in enumerate(prog_bar):\n",
    "        optimizer.zero_grad()\n",
    "        images, targets = data\n",
    "        \n",
    "        images = list(image.to(DEVICE) for image in images)\n",
    "        \n",
    "        ###this'll only work with collate_fn\n",
    "        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "        ###don't loop through targets as above because its single dict and not list, if try to loop then throws error as 'str' doesn't has items()\n",
    "        ###this'll only work when there is no collate_fn but without this, error will be thrown wrt to size as \"required (N,4) but got (4,1,4)\"\n",
    "        ###can check in this link : https://discuss.pytorch.org/t/valueerror-expected-target-boxes-to-be-a-tensorof-shape-n-4-got-torch-size-4/137314\n",
    "        #targets = [{k: v.to(DEVICE) for k, v in targets.items()}] \n",
    "\n",
    "        #model.train()\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_value = losses.item()\n",
    "        train_loss_list.append(loss_value)\n",
    "        train_loss_hist.send(loss_value)\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        train_itr += 1\n",
    "    \n",
    "        ###update the loss value beside the progress bar for each iteration\n",
    "        prog_bar.set_description(desc=f\"Loss: {loss_value:.4f}\")\n",
    "\n",
    "    return train_loss_list\n",
    "\n",
    "###function for running validation iterations\n",
    "def validate(valid_data_loader, model):\n",
    "    print('Validating...')\n",
    "    global val_itr\n",
    "    global val_loss_list\n",
    "    \n",
    "    ###initialize tqdm progress bar\n",
    "    prog_bar = tqdm(valid_data_loader, total=len(valid_data_loader))\n",
    "    \n",
    "    for i, data in enumerate(prog_bar):\n",
    "        images, targets = data\n",
    "        \n",
    "        images = list(image.to(DEVICE) for image in images)\n",
    "        \n",
    "        ###this'll only work with collate_fn\n",
    "        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "        ###don't loop through targets as above because its single dict and not list, if try to loop then throws error as 'str' doesn't has items()\n",
    "        ###this'll only work when there is no collate_fn but without this, error will be thrown wrt to size as \"required (N,4) but got (4,1,4)\"\n",
    "        ###can check in this link : https://discuss.pytorch.org/t/valueerror-expected-target-boxes-to-be-a-tensorof-shape-n-4-got-torch-size-4/137314\n",
    "        #targets = [{k: v.to(DEVICE) for k, v in targets.items()}]   \n",
    "        \n",
    "        ###not to calculate gradiemts as it affects backward propogation and if removed for other than training then will give forward pass error\n",
    "        with torch.no_grad():\n",
    "            loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_value = losses.item()\n",
    "        val_loss_list.append(loss_value)\n",
    "        val_loss_hist.send(loss_value)\n",
    "        val_itr += 1\n",
    "        ###update the loss value beside the progress bar for each iteration\n",
    "        prog_bar.set_description(desc=f\"Loss: {loss_value:.4f}\")\n",
    "    return val_loss_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e63587-24c2-4dbe-bb74-af396e989b84",
   "metadata": {},
   "source": [
    "TorchMetrics Functions - mAP & IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce02c8a2-87eb-49a6-b62d-94bae5f81192",
   "metadata": {},
   "outputs": [],
   "source": [
    "###functions for getting mAP & IoU using torchmetrics\n",
    "def meanAvgPrecision(data_loader, model, type):\n",
    "    if(type=='train'):\n",
    "        print(\"Calculating mAP for training data...\")\n",
    "    elif(type=='valid'):\n",
    "        print(\"Calculating mAP for validation data...\")\n",
    "    \n",
    "    global mAP_train_itr\n",
    "    global mAP_train_list\n",
    "    global mAP_valid_itr\n",
    "    global mAP_valid_list\n",
    "    \n",
    "    for i, data in enumerate(data_loader):\n",
    "        images, targets = data\n",
    "            \n",
    "        images = list(image.to(DEVICE) for image in images)\n",
    "        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "    \n",
    "        ###not to calculate gradiemts as it affects backward propogation and if removed for other than training then will give forward pass error\n",
    "        with torch.no_grad():\n",
    "            #model.eval()\n",
    "            model_pred = model(images)\n",
    "        \n",
    "        metric_mAP = MeanAveragePrecision(iou_type=\"bbox\")\n",
    "        metric_mAP.update(model_pred, targets)\n",
    "        mAP = metric_mAP.compute()\n",
    "        ###getting only 'map' and not other metrics, refer https://torchmetrics.readthedocs.io/en/stable/detection/mean_average_precision.html\n",
    "        '''other metrics are :  {'classes': tensor(0, dtype=torch.int32),\n",
    "                             'map': tensor(0.6000),\n",
    "                             'map_50': tensor(1.),\n",
    "                             'map_75': tensor(1.),\n",
    "                             'map_large': tensor(0.6000),\n",
    "                             'map_medium': tensor(-1.),\n",
    "                             'map_per_class': tensor(-1.),\n",
    "                             'map_small': tensor(-1.),\n",
    "                             'mar_1': tensor(0.6000),\n",
    "                             'mar_10': tensor(0.6000),\n",
    "                             'mar_100': tensor(0.6000),\n",
    "                             'mar_100_per_class': tensor(-1.),\n",
    "                             'mar_large': tensor(0.6000),\n",
    "                             'mar_medium': tensor(-1.),\n",
    "                             'mar_small': tensor(-1.)}'''\n",
    "        mAP_val = mAP.get('map').item()\n",
    "        mAP_val = 0 if(math.isnan(mAP_val)) else mAP_val   ###to avoid getting nan as result after calculations\n",
    "        ###based on type as training data or validation data\n",
    "        if(type=='train'):\n",
    "            mAP_train_list.append(mAP_val)\n",
    "            mAP_train_hist.send(mAP_val)\n",
    "            mAP_train_itr += 1\n",
    "        elif(type=='valid'):\n",
    "            mAP_valid_list.append(mAP_val)\n",
    "            mAP_valid_hist.send(mAP_val)\n",
    "            mAP_valid_itr += 1\n",
    "\n",
    "    ###return list based on type\n",
    "    if(type=='train'):\n",
    "        return mAP_train_list\n",
    "    elif(type=='valid'):\n",
    "        return mAP_valid_list\n",
    "\n",
    "def InteroverUnion(data_loader, model, type):\n",
    "    if(type=='train'):\n",
    "        print(\"Calculating IoU for training data...\")\n",
    "    elif(type=='valid'):\n",
    "        print(\"Calculating IoU for validation data...\")\n",
    "    \n",
    "    global IoU_train_itr\n",
    "    global IoU_train_list\n",
    "    global IoU_valid_itr\n",
    "    global IoU_valid_list\n",
    "    \n",
    "    for i, data in enumerate(data_loader):\n",
    "        images, targets = data\n",
    "            \n",
    "        images = list(image.to(DEVICE) for image in images)\n",
    "        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        ###not to calculate gradiemts as it affects backward propogation and if removed for other than training then will give forward pass error\n",
    "        with torch.no_grad():\n",
    "            #model.eval()\n",
    "            model_pred = model(images)\n",
    "        \n",
    "        metric_IoU = IntersectionOverUnion()\n",
    "        IoU = metric_IoU(model_pred, targets)\n",
    "        ###only 1 metric as 'iou' unlike mAP\n",
    "        IoU_val = IoU.get('iou').item()\n",
    "        IoU_val = 0 if(math.isnan(IoU_val)) else IoU_val   ###to avoid getting nan as result after calculations\n",
    "        ###based on type as training data or validation data\n",
    "        if(type=='train'):\n",
    "            IoU_train_list.append(IoU_val)\n",
    "            IoU_train_hist.send(IoU_val)\n",
    "            IoU_train_itr += 1\n",
    "        elif(type=='valid'):\n",
    "            IoU_valid_list.append(IoU_val)\n",
    "            IoU_valid_hist.send(IoU_val)\n",
    "            IoU_valid_itr += 1\n",
    "\n",
    "    ###return list based on type\n",
    "    if(type=='train'):\n",
    "        return IoU_train_list\n",
    "    elif(type=='valid'):\n",
    "        return IoU_valid_list\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644c0eee-4fa4-4796-9795-3df36c437fce",
   "metadata": {},
   "source": [
    "Load Checkpoint to Continue Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221864c9-7fb7-4a87-b51b-0e5084431963",
   "metadata": {},
   "outputs": [],
   "source": [
    "###function to Load Checkpoint to Continue Training Process\n",
    "def load_last_checkpoint(model, optimizer, ckpt_path):\n",
    "    ###Input model & optimizer should be pre-defined, this routine only updates their states\n",
    "    start_epoch = 0\n",
    "    if os.path.isfile(ckpt_path):\n",
    "        print(\"Loading Checkpoint '{}'\".format(ckpt_path))\n",
    "        checkpoint = torch.load(ckpt_path)\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        train_loss_list = checkpoint['train_loss_list']\n",
    "        val_loss_list = checkpoint['val_loss_list']\n",
    "        mAP_train_list = checkpoint['mAP_train_list']\n",
    "        IoU_train_list = checkpoint['IoU_train_list']\n",
    "        mAP_valid_list = checkpoint['mAP_valid_list']\n",
    "        IoU_valid_list = checkpoint['IoU_valid_list']\n",
    "        print(\"Loaded Checkpoint '{}' (epoch {})\".format(ckpt_path, start_epoch))\n",
    "    else:\n",
    "        print(\"No Checkpoint found at '{}'\".format(ckpt_path))\n",
    "\n",
    "    return model, optimizer, start_epoch, train_loss_list, val_loss_list, mAP_train_list, IoU_train_list, mAP_valid_list, IoU_valid_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9e66fa-5b84-4792-9309-b94642381cc9",
   "metadata": {},
   "source": [
    "Graph/Plot Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49adf2b-d648-4563-9936-47e837414d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "###function to plot graphs for losses and metrics like mAP & IoU for both training and validation data\n",
    "def plot_metrics_graph(type_list, type_color, type_name, OUT_DIR, epoch):\n",
    "    ###create 6 subplots, one for each, training and validation losses & metrics - mAP, IoU\n",
    "    figure_1, plot_ax = plt.subplots()\n",
    "\n",
    "    plot_ax.plot(type_list, color=type_color)\n",
    "    plot_ax.set_xlabel('iterations')\n",
    "    plot_ax.set_ylabel(type_name)\n",
    "    figure_1.savefig(f\"{OUT_DIR}\\\\FRCNN_CustomBackbone_Outputs\\\\Model_Graphs_FRCNN_CustomBackbone\\\\{type_name}_{epoch+1}.png\")\n",
    "    \n",
    "    #print('SAVING PLOTS COMPLETE...')\n",
    "    \n",
    "    plt.close('all')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e573a84-49eb-40d3-ac45-5e31cff3808a",
   "metadata": {},
   "source": [
    "Training and Validation Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e90cfc-52ab-4c84-b2f1-bbc148ac70da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    ###initializing the iterators\n",
    "    train_itr = 1\n",
    "    val_itr = 1\n",
    "    mAP_train_itr = 1\n",
    "    IoU_train_itr = 1\n",
    "    mAP_valid_itr = 1\n",
    "    IoU_valid_itr = 1\n",
    "    \n",
    "    ###train and validation loss lists to store loss values of all iterations till ena and plot graphs for all iterations\n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "    mAP_train_list = []\n",
    "    IoU_train_list = []\n",
    "    mAP_valid_list = []\n",
    "    IoU_valid_list = []\n",
    "\n",
    "    ###initialize the Averager class\n",
    "    train_loss_hist = Averager()\n",
    "    val_loss_hist = Averager()\n",
    "    mAP_train_hist = Averager()\n",
    "    IoU_train_hist = Averager()\n",
    "    mAP_valid_hist = Averager()\n",
    "    IoU_valid_hist = Averager()\n",
    "    \n",
    "    ###initialize the model and move model to the computation device\n",
    "    model = create_model_frcnn_custombackbone(num_classes=NUM_CLASSES)\n",
    "    model = model.to(DEVICE)\n",
    "    ###get the model parameters\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    ###define the optimizer\n",
    "    optimizer = torch.optim.SGD(params, lr=0.001, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "    ###get the latest saved checkpoint if available to continue the training process\n",
    "    last_ckpt_path_list = glob.glob(OUT_DIR+\"\\\\FRCNN_CustomBackbone_Outputs\\\\Model_Graphs_FRCNN_CustomBackbone\\\\*.pt\")\n",
    "    #print(\"last_ckpt_path_list = = =\",last_ckpt_path_list)\n",
    "    if(len(last_ckpt_path_list) > 0):\n",
    "        last_ckpt_path = max(last_ckpt_path_list, key=os.path.getctime)   ###to get the latest saved ckpt\n",
    "\n",
    "        ###if ckpt available only then call the function to get last ckpt state\n",
    "        model, optimizer, START_EPOCH, train_loss_list, val_loss_list, mAP_train_list, IoU_train_list, mAP_valid_list, IoU_valid_list = load_last_checkpoint(model, optimizer, last_ckpt_path)\n",
    "        \n",
    "    ###start the training epochs\n",
    "    for epoch in range(START_EPOCH, NUM_EPOCHS):\n",
    "        \n",
    "        print(f\"\\nEPOCH {epoch+1} of {NUM_EPOCHS}\")\n",
    "        \n",
    "        ###reset the training and validation loss histories for the current epoch\n",
    "        train_loss_hist.reset()\n",
    "        val_loss_hist.reset()\n",
    "        mAP_train_hist.reset()\n",
    "        IoU_train_hist.reset()\n",
    "        mAP_valid_hist.reset()\n",
    "        IoU_valid_hist.reset()\n",
    "        \n",
    "        ###start timer and carry out training and validation\n",
    "        start = time.time()\n",
    "\n",
    "        ###put the model back to training state after the 1st epoch is completed and after model is evaluated to get metrics - mAP & IoU\n",
    "        ###if not put back to training state after evaluating state is completed then will throw error while processing the dataloader\n",
    "        if(epoch > 0):\n",
    "            model.train()\n",
    "        ###calling training and validation func\n",
    "        train_loss = train(train_loader, model)\n",
    "        val_loss = validate(valid_loader, model)\n",
    "\n",
    "        ###put the model to evaluate state for calculating the metrics - mAP & IoU\n",
    "        model.eval()\n",
    "        ###calling metrics - mAP & IoU func for training and validation\n",
    "        mAP_train = meanAvgPrecision(train_loader, model, 'train')\n",
    "        IoU_train = InteroverUnion(train_loader, model, 'train')\n",
    "        mAP_valid = meanAvgPrecision(valid_loader, model, 'valid')\n",
    "        IoU_valid = InteroverUnion(valid_loader, model, 'valid')\n",
    "        \n",
    "        print(f\"Epoch #{epoch+1} train loss: {train_loss_hist.value:.3f}\")   \n",
    "        print(f\"Epoch #{epoch+1} validation loss: {val_loss_hist.value:.3f}\")\n",
    "        print(f\"Epoch #{epoch+1} mAP_train: {mAP_train_hist.value:.3f}\")   \n",
    "        print(f\"Epoch #{epoch+1} iou_train: {IoU_train_hist.value:.3f}\")\n",
    "        print(f\"Epoch #{epoch+1} mAP_valid: {mAP_valid_hist.value:.3f}\")   \n",
    "        print(f\"Epoch #{epoch+1} iou_valid: {IoU_valid_hist.value:.3f}\") \n",
    "        \n",
    "        end = time.time()\n",
    "        print(f\"Took {((end - start) / 60):.3f} minutes for epoch {epoch+1}\")\n",
    "\n",
    "        ###get different states of model in dict like optimizer, model and next epoch because current epoch will be saved and next time retraining shouldn't happen\n",
    "        ###also save the losses and metrics such that they can be continued to plot graphs\n",
    "        model_state = {'epoch': epoch + 1, 'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict(),\n",
    "                       'train_loss_list' : train_loss_list, 'val_loss_list' : val_loss_list, 'mAP_train_list' : mAP_train_list,\n",
    "                       'IoU_train_list' : IoU_train_list, 'mAP_valid_list' : mAP_valid_list, 'IoU_valid_list' : IoU_valid_list}\n",
    "        \n",
    "        if (epoch+1) % SAVE_MODEL_EPOCH == 0:   ###save model after every n epochs\n",
    "            torch.save(model_state, f\"{OUT_DIR}\\\\FRCNN_CustomBackbone_Outputs\\\\Model_Graphs_FRCNN_CustomBackbone\\\\{MODEL_NAME}_ckpt_{epoch+1}.pt\")   ###save as pt and not pth because collides with python path library\n",
    "            print('SAVING MODEL COMPLETE...\\n')\n",
    "        \n",
    "        if (epoch+1) % SAVE_PLOTS_EPOCH == 0:   ###save 6 subplots, one for each, training and validation losses & metrics - mAP, IoU after n epochs\n",
    "            plot_metrics_graph(train_loss, 'red', 'train_loss', OUT_DIR, epoch)\n",
    "            plot_metrics_graph(val_loss, 'blue', 'valid_loss', OUT_DIR, epoch)\n",
    "            plot_metrics_graph(mAP_train, 'red', 'mAP_train', OUT_DIR, epoch)\n",
    "            plot_metrics_graph(mAP_valid, 'blue', 'mAP_valid', OUT_DIR, epoch)\n",
    "            plot_metrics_graph(IoU_train, 'red', 'IoU_train', OUT_DIR, epoch)\n",
    "            plot_metrics_graph(IoU_valid, 'blue', 'IoU_valid', OUT_DIR, epoch)\n",
    "\n",
    "            print('SAVING PLOTS COMPLETE...')\n",
    "\n",
    "        if (epoch+1) == NUM_EPOCHS:   ###save loss plots and model once at the end\n",
    "            torch.save(model_state, f\"{OUT_DIR}\\\\FRCNN_CustomBackbone_Outputs\\\\Model_Graphs_FRCNN_CustomBackbone\\\\{MODEL_NAME}_ckpt_{epoch+1}.pt\")   ###save as pt and not pth because collides with python path library\n",
    "            print('SAVING MODEL FOR LAST EPOCH COMPLETE...\\n')\n",
    "            \n",
    "            plot_metrics_graph(train_loss, 'red', 'train_loss', OUT_DIR, epoch)\n",
    "            plot_metrics_graph(val_loss, 'blue', 'valid_loss', OUT_DIR, epoch)\n",
    "            plot_metrics_graph(mAP_train, 'red', 'mAP_train', OUT_DIR, epoch)\n",
    "            plot_metrics_graph(mAP_valid, 'blue', 'mAP_valid', OUT_DIR, epoch)\n",
    "            plot_metrics_graph(IoU_train, 'red', 'IoU_train', OUT_DIR, epoch)\n",
    "            plot_metrics_graph(IoU_valid, 'blue', 'IoU_valid', OUT_DIR, epoch)\n",
    "\n",
    "            print('SAVING PLOTS FOR LAST EPOCH COMPLETE...')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccc12be-0440-458f-b8c1-cd5164c14ee1",
   "metadata": {},
   "source": [
    "IoU, Avg IoU, Precision, Recall Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043250ed-d39e-416d-b31d-464d08040878",
   "metadata": {},
   "outputs": [],
   "source": [
    "###function to calculate IoU, Avg IoU, Precision, Recall\n",
    "def IoU_Calc(grndtrth, pred):\n",
    "\t###determine the (x, y)-coordinates of the intersection rectangle\n",
    "\tx_grndtrth = max(grndtrth[0], pred[0])\n",
    "\ty_grndtrth = max(grndtrth[1], pred[1])\n",
    "\tx_pred = min(grndtrth[2], pred[2])\n",
    "\ty_pred = min(grndtrth[3], pred[3])\n",
    "\t\n",
    "    ###compute the area of intersection rectangle\n",
    "\tinterArea = max(0, x_pred - x_grndtrth + 1) * max(0, y_pred - y_grndtrth + 1)\n",
    "\t###compute the area of both the prediction and ground-truth rectangles\n",
    "\tgrndtrth_Area = (grndtrth[2] - grndtrth[0] + 1) * (grndtrth[3] - grndtrth[1] + 1)\n",
    "\tpred_Area = (pred[2] - pred[0] + 1) * (pred[3] - pred[1] + 1)\n",
    "\t###compute the IoU by taking the intersection area and dividing it by the sum of prediction + ground-truth areas - the interesection area\n",
    "\tIoU = interArea / float(grndtrth_Area + pred_Area - interArea)\n",
    "\t# return the intersection over union value\n",
    "\treturn IoU\n",
    "\n",
    "def IoU_Precision_Recall_F1(metrics_dict, CLASSES):\n",
    "    for cls in CLASSES:\n",
    "        if(cls != \"background\"):\n",
    "            ###each class IoU\n",
    "            if((cls+\"_IoU\" in metrics_dict) and (cls+\"_Count\" in metrics_dict)):\n",
    "                metrics_dict[cls+\"_IoU\"] = round(metrics_dict.get(cls+\"_IoU\") / metrics_dict.get(cls+\"_Count\"),2)\n",
    "            ###each class precision = ((TP) / (TP + FP))\n",
    "            if((cls+\"_TP\" in metrics_dict) and (cls+\"_FP\" in metrics_dict)):\n",
    "                metrics_dict[cls+\"_Precision\"] = round(((metrics_dict.get(cls+\"_TP\")) / ((metrics_dict.get(cls+\"_TP\")) + (metrics_dict.get(cls+\"_FP\")))),2)\n",
    "            ###each class recall = ((TP) / (TP + FN))\n",
    "            ###FN is when model doesn't detects a groundtruth due to < threshold\n",
    "            '''###here FN is other class's FP because its binary classification only when model predicts all based on threshold'''\n",
    "            if((cls+\"_TP\" in metrics_dict) and (cls+\"_FN\" in metrics_dict)):\n",
    "                metrics_dict[cls+\"_Recall\"] = round(((metrics_dict.get(cls+\"_TP\")) / ((metrics_dict.get(cls+\"_TP\")) + (metrics_dict.get(cls+\"_FN\")))),2)\n",
    "            ###each class F1 = (2 / ((1 / precision) + (1 / recall)))\n",
    "            if((cls+\"_Precision\" in metrics_dict) and (cls+\"_Recall\" in metrics_dict)):\n",
    "                metrics_dict[cls+\"_F1\"] = round((2 / ((1 / (metrics_dict.get(cls+\"_Precision\"))) + (1 / (metrics_dict.get(cls+\"_Recall\"))))),2)\n",
    "\n",
    "    ###both class IoU i.e., Average\n",
    "    metrics_dict[\"Avg_IoU\"] = round(((metrics_dict.get(str(CLASSES[1])+\"_IoU\") + metrics_dict.get(str(CLASSES[2])+\"_IoU\")) / 2),2)\n",
    "    ###both class IoU i.e., Average\n",
    "    metrics_dict[\"Avg_Precision\"] = round(((metrics_dict.get(str(CLASSES[1])+\"_Precision\") + metrics_dict.get(str(CLASSES[2])+\"_Precision\")) / 2),2)\n",
    "    ###both class IoU i.e., Average\n",
    "    metrics_dict[\"Avg_Recall\"] = round(((metrics_dict.get(str(CLASSES[1])+\"_Recall\") + metrics_dict.get(str(CLASSES[2])+\"_Recall\")) / 2),2)\n",
    "    ###both class IoU i.e., Average\n",
    "    metrics_dict[\"Avg_F1\"] = round(((metrics_dict.get(str(CLASSES[1])+\"_F1\") + metrics_dict.get(str(CLASSES[2])+\"_F1\")) / 2),2)\n",
    "\n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f799684e-37a4-42fe-b1d2-52578ea8163b",
   "metadata": {},
   "source": [
    "Model Testing - Test Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bfbf8d-fb2d-44f2-801a-5341037319f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_time = time.time()\n",
    "###set the computation device\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "###load the model and the trained weights\n",
    "model = create_model_frcnn_custombackbone(num_classes=3).to(device)\n",
    "###if model saved with other parameters like optimizer, losses, epoch, etc., then below method won't load the model\n",
    "'''model.load_state_dict(torch.load(\n",
    "    OUT_DIR+\"\\\\Model_Graphs\\\\\"+MODEL_NAME+\"_48.pt\", map_location=device\n",
    "))'''\n",
    "###to load the model correctly, get the model's state dict and then use/run it\n",
    "model.load_state_dict(torch.load(\n",
    "    OUT_DIR+\"\\\\FRCNN_CustomBackbone_Outputs\\\\Model_Graphs_FRCNN_CustomBackbone\\\\\"+MODEL_NAME+\"_ckpt_48.pt\", map_location=device\n",
    ")['state_dict'])   ###saved as pt and not pth because collides with python path library\n",
    "###get the model into evaluation mode\n",
    "model.eval()\n",
    "\n",
    "###csv name to store metrics\n",
    "out_csv = MODEL_NAME + \"_MetricsCSV_ckpt48.csv\"\n",
    "###directory where all the images and its annotation json are present\n",
    "DIR_TEST = \"G:\\\\Projects\\\\ADAS\\\\DDW\\\\Data\\\\Usable_Data\\\\test\"\n",
    "test_images = glob.glob(f\"{DIR_TEST}/*.jpg\")\n",
    "print(f\"Test instances: {len(test_images)}\")\n",
    "\n",
    "###read the coco json to get all the ids, classes/annotations, boxes to make as groundtruth for metrics calculation : TP, FP, FN, Precision, Recall, F1 etc.\n",
    "coco = COCO(DIR_TEST + \"\\\\test_3_annotations.coco.json\")\n",
    "\n",
    "ids_l = list(sorted(coco.imgs.keys()))\n",
    "imgname_l = [coco.loadImgs([i])[0]['file_name'] for i in ids_l]   ###[0] used because for 1 id only 1 image and 1 class annotation, if multiole then [0]...[n]\n",
    "ann_ids_l = [coco.getAnnIds(imgIds=ids_l[i])[0] for i in ids_l]\n",
    "ann_cls_l = [coco.loadAnns(i)[0]['category_id'] for i in ann_ids_l]\n",
    "bbox_l = [coco.loadAnns(i)[0]['bbox'] for i in ann_ids_l]\n",
    "\n",
    "###classes: 0 index is reserved for background\n",
    "CLASSES = ['background', 'awake', 'drowsy']\n",
    "###define the detection threshold, any detection having score below this will be discarded\n",
    "detection_threshold = 0.75\n",
    "###dictionary to store count, TP, FP, FN, IoU, Precision, Recall, F1 of each class and avg of all\n",
    "metrics_dict = {}\n",
    "for cls in CLASSES:\n",
    "    if cls != \"background\":\n",
    "        metrics_dict[cls+\"_Count\"] = 0\n",
    "        metrics_dict[cls+\"_TP\"] = 0\n",
    "        metrics_dict[cls+\"_FP\"] = 0\n",
    "        metrics_dict[cls+\"_FN\"] = 0\n",
    "        metrics_dict[cls+\"_IoU\"] = 0\n",
    "        metrics_dict[cls+\"_Precision\"] = 0\n",
    "        metrics_dict[cls+\"_Recall\"] = 0\n",
    "        metrics_dict[cls+\"_F1\"] = 0\n",
    "metrics_dict[\"Avg_IoU\"] = 0\n",
    "metrics_dict[\"Avg_Precision\"] = 0\n",
    "metrics_dict[\"Avg_Recall\"] = 0\n",
    "metrics_dict[\"Avg_F1\"] = 0\n",
    "\n",
    "###loop through all test images\n",
    "for i in range(len(test_images)):\n",
    "    each_img_time = time.time()\n",
    "    ###get the image file name for saving output later on\n",
    "    image_name = test_images[i].split('\\\\')[-1]\n",
    "    print(\"Image Name = \", image_name)\n",
    "    ###read the image using cv2\n",
    "    image = cv2.imread(test_images[i])\n",
    "    ###get image size for further groundtruth box resizing\n",
    "    hght, wdth, *_ = image.shape\n",
    "    ###resize the image probably to the model trained size to make the detections faster\n",
    "    #image = cv2.resize(image, (RESIZE_TO, RESIZE_TO))\n",
    "    ###take a copy of image to draw boxes and classes name with score\n",
    "    orig_image = image.copy()\n",
    "    ###BGR to RGB\n",
    "    image = cv2.cvtColor(orig_image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "    ###make the pixel range between 0 and 1\n",
    "    image /= 255.0\n",
    "    ###bring color channels to front followed by width and height\n",
    "    image = np.transpose(image, (2, 0, 1)).astype(np.float32)   ###np.float isn't supported in numpy==1.22 so np.float64 or float32 which is equivalant\n",
    "    ###convert to tensor\n",
    "    image = torch.tensor(image, dtype=torch.float).cuda()\n",
    "    ###add batch dimension\n",
    "    image = torch.unsqueeze(image, 0)\n",
    "\n",
    "    ###get the index of the matching image name from coco ids list to get other info like class, box for current image(which acts as groundtruth)\n",
    "    cur_id_index = imgname_l.index(image_name)\n",
    "    cur_ann_cls = ann_cls_l[cur_id_index]\n",
    "    cur_box = bbox_l[cur_id_index]\n",
    "    ###to convert width and height of box into xmax/x2, ymax/y2 and not to resize based on model trained image size i.e., 416 because model will give outputs matching with original size images\n",
    "    cur_box[2] = cur_box[0] + cur_box[2]\n",
    "    cur_box[3] = cur_box[1] + cur_box[3]\n",
    "    '''cur_box[0] = (cur_box[0]/wdth)*416\n",
    "    cur_box[2] = (cur_box[0] + cur_box[2]/wdth)*416\n",
    "    cur_box[1] = (cur_box[1]/hght)*416\n",
    "    cur_box[3] = (cur_box[1] + cur_box[3]/hght)*416'''\n",
    "\n",
    "    ###not to calculate gradiemts as it affects backward propogation and if removed for other than training then will give forward pass error\n",
    "    with torch.no_grad():\n",
    "        ###Note : model prediction on default size images without resizing the image as model will resize internally and give the outputs matching with original size images\n",
    "        outputs = model(image)\n",
    "    \n",
    "    ###load all detection to CPU for further operations\n",
    "    outputs = [{k: v.to('cpu') for k, v in t.items()} for t in outputs]\n",
    "    \n",
    "    ###draw boxes, classes with score on images if above threshold and if boxes available\n",
    "    if len(outputs[0]['boxes']) != 0:\n",
    "        ###converting tensors to numpy\n",
    "        boxes = outputs[0]['boxes'].data.numpy()\n",
    "        scores = outputs[0]['scores'].data.numpy()\n",
    "        ###getting the max score above threshold from multiple boxes as only 1 box is required, works for custom backbone where single prediction can't be defined unlike pretrained backbone(given as parameter)\n",
    "        max_score = scores.max(where=(scores >= detection_threshold),initial=0)\n",
    "        max_score_index = np.where(scores==max_score)\n",
    "        ###filter out boxes according to `detection_threshold`, Note: similar to NMS but in FRCNN, NMS is internal\n",
    "        #boxes = boxes[scores >= detection_threshold].astype(np.int32)   ###works for frcnn with pretrained backbones having only 1 detection(given as parameter)\n",
    "        boxes = boxes[max_score_index].astype(np.int32) if(max_score >= detection_threshold) else []   ###if no single box matching threshold then empty\n",
    "        draw_boxes = boxes.copy()\n",
    "        ###get all the predicited class names\n",
    "        pred_classes = [CLASSES[i] for i in outputs[0]['labels'].cpu().numpy()]\n",
    "        \n",
    "        ###draw the bounding boxes and write the class name with score on top of it\n",
    "        for j, box in enumerate(draw_boxes):\n",
    "            \n",
    "            ###get the counts of TP, FP for each class in dict\n",
    "            if(CLASSES[cur_ann_cls] == pred_classes[j]):   ###TP case(score is already > threshold)\n",
    "                metrics_dict[pred_classes[j]+'_TP'] = metrics_dict.get(pred_classes[j]+'_TP')+1\n",
    "            else:   ###FP case(score is already > threshold)\n",
    "                metrics_dict[pred_classes[j]+'_FP'] = metrics_dict.get(pred_classes[j]+'_FP')+1\n",
    "            ###get the avg IoU for each class in dict\n",
    "            cur_IoU = IoU_Calc(cur_box, box)\n",
    "            metrics_dict[pred_classes[j]+'_IoU'] = round((metrics_dict.get(pred_classes[j]+'_IoU') + cur_IoU),2)\n",
    "\n",
    "            ###draw groundtruth on images\n",
    "            cv2.rectangle(orig_image,\n",
    "                        (int(cur_box[0]), int(cur_box[1])),\n",
    "                        (int(cur_box[2]), int(cur_box[3])),\n",
    "                        (0, 0, 255), 2)\n",
    "            cv2.putText(orig_image, CLASSES[cur_ann_cls], \n",
    "                        (int(cur_box[0]), int(cur_box[3]+10)),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), \n",
    "                        2, lineType=cv2.LINE_AA)\n",
    "            \n",
    "            ###draw model predictions on image\n",
    "            cv2.rectangle(orig_image,\n",
    "                        (int(box[0]), int(box[1])),\n",
    "                        (int(box[2]), int(box[3])),\n",
    "                        (0, 255, 0,), 2)\n",
    "            cv2.putText(orig_image, pred_classes[j], \n",
    "                        (int(box[0]), int(box[1]-5)),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), \n",
    "                        2, lineType=cv2.LINE_AA)\n",
    "            cv2.putText(orig_image, str(round(scores[j],2)), \n",
    "                        (int(box[0]), int(box[1]-20)),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), \n",
    "                        2, lineType=cv2.LINE_AA)\n",
    "        \n",
    "        #cv2.imshow('Prediction', orig_image)\n",
    "        #cv2.waitKey(1)\n",
    "\n",
    "        ###get the counts of FN, when model doesn't detects a groundtruth due to < threshold\n",
    "        metrics_dict[CLASSES[cur_ann_cls]+'_FN'] = metrics_dict.get(CLASSES[cur_ann_cls]+'_FN')+1 if(len(draw_boxes) == 0) else metrics_dict.get(CLASSES[cur_ann_cls]+'_FN')      \n",
    "        \n",
    "        ###save the predicted images\n",
    "        path = OUT_DIR + \"\\\\FRCNN_CustomBackbone_Outputs\\\\Test_Predictions_pt_FRCNN_CustomBackbone\\\\\" + image_name\n",
    "        cv2.imwrite(path, orig_image)\n",
    "        \n",
    "    ###get the counts of FN, when model doesn't detects a groundtruth at all\n",
    "    metrics_dict[CLASSES[cur_ann_cls]+'_FN'] = metrics_dict.get(CLASSES[cur_ann_cls]+'_FN')+1 if(len(outputs[0]['boxes']) == 0) else metrics_dict.get(CLASSES[cur_ann_cls]+'_FN')\n",
    "    ###get the counts of each groundtruth class in dict\n",
    "    metrics_dict[CLASSES[cur_ann_cls]+'_Count'] = metrics_dict.get(CLASSES[cur_ann_cls]+'_Count')+1  \n",
    "\n",
    "    print(f\"Image {i+1} Done with Time : \", str(time.time() - each_img_time))\n",
    "    print('-'*50)\n",
    "\n",
    "'''\n",
    "###1 class's FP is another class's FN in binary classification if background class is neglected, so swap the FPs for FNs only when model predicts all above threshold\n",
    "metrics_dict[str(CLASSES[1])+\"_FN\"] = metrics_dict.get(CLASSES[2]+'_FP')\n",
    "metrics_dict[str(CLASSES[2])+\"_FN\"] = metrics_dict.get(CLASSES[1]+'_FP')\n",
    "'''\n",
    "###to calculate the avg IoU\n",
    "metrics_dict = IoU_Precision_Recall_F1(metrics_dict, CLASSES)\n",
    "print(\"\\nDifferent Metrics = \", metrics_dict)\n",
    "\n",
    "###save the metrics in csv using pandas\n",
    "df = pd.DataFrame([metrics_dict]) \n",
    "df.to_csv (OUT_DIR + \"\\\\FRCNN_CustomBackbone_Outputs\\\\Test_Predictions_pt_FRCNN_CustomBackbone\\\\\" + out_csv, index=False, header=True)\n",
    "\n",
    "#cv2.destroyAllWindows()\n",
    "print('\\nTEST PREDICTIONS COMPLETE WITH TIME : ',str(time.time() - all_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684d2075-95f5-4a9f-b5b8-f185b0b140aa",
   "metadata": {},
   "source": [
    "Model Testing - Video/Webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02e6930-84fc-450e-bfa1-e8feb9e7d093",
   "metadata": {},
   "outputs": [],
   "source": [
    "###set the computation device\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "###load the model and the trained weights\n",
    "model = create_model_frcnn_custombackbone(num_classes=3).to(device)\n",
    "###if model saved with other parameters like optimizer, losses, epoch, etc., then below method won't load the model\n",
    "'''model.load_state_dict(torch.load(\n",
    "    OUT_DIR+\"\\\\Model_Graphs\\\\\"+MODEL_NAME+\"_48.pt\", map_location=device\n",
    "))'''\n",
    "###to load the model correctly, get the model's state dict and then use/run it\n",
    "model.load_state_dict(torch.load(\n",
    "    OUT_DIR+\"\\\\FRCNN_CustomBackbone_Outputs\\\\Model_Graphs_FRCNN_CustomBackbone\\\\\"+MODEL_NAME+\"_ckpt_48.pt\", map_location=device\n",
    ")['state_dict'])   ###saved as pt and not pth because collides with python path library\n",
    "###get the model into evaluation mode\n",
    "model.eval()\n",
    "\n",
    "###classes: 0 index is reserved for background\n",
    "CLASSES = ['background', 'awake', 'drowsy']\n",
    "###define the detection threshold, any detection having score below this will be discarded\n",
    "detection_threshold = 0.75\n",
    "\n",
    "###initialize the video stream\n",
    "vid_strm = cv2.VideoCapture(\"https://192.168.0.104:8080/video\")   ###from android app webcam else give src=0 for local cam\n",
    "###save the video stream used for predictions\n",
    "vid_strm_save = cv2.VideoWriter(OUT_DIR + \"\\\\FRCNN_CustomBackbone_Outputs\\\\Video_Output_pt_FRCNN_CustomBackbone\\\\\" + MODEL_NAME + \"ckpt48_webcam.mp4\",cv2.VideoWriter_fourcc(*'MP4V'),\n",
    "                         10,(1920,1080))\n",
    "\n",
    "###initialize the FPS counter\n",
    "fps = FPS().start()\n",
    "\n",
    "#while True:\n",
    "while(vid_strm.isOpened()):\n",
    "    ###read/get the frame/image from video stream/webcam\n",
    "    val, image = vid_strm.read()\n",
    "    #image = cv2.resize(image, (RESIZE_TO, RESIZE_TO))   ###if resized, model will run faster and fps increases\n",
    "    ###if no frame then exit the process\n",
    "    if image is None:\n",
    "        break\n",
    "    ###take a copy of image to draw boxes and classes name with score\n",
    "    orig_image = image.copy()\n",
    "    ###BGR to RGB\n",
    "    image = cv2.cvtColor(orig_image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "    ###make the pixel range between 0 and 1\n",
    "    image /= 255.0\n",
    "    ###bring color channels to front followed by width and height\n",
    "    image = np.transpose(image, (2, 0, 1)).astype(np.float32)   ###np.float isn't supported in numpy==1.22 so np.float64 or float32 which is equivalant\n",
    "    ###convert to tensor\n",
    "    image = torch.tensor(image, dtype=torch.float).cuda()\n",
    "    ###add batch dimension\n",
    "    image = torch.unsqueeze(image, 0)\n",
    "\n",
    "    ###not to calculate gradiemts as it affects backward propogation and if removed for other than training then will give forward pass error\n",
    "    with torch.no_grad():\n",
    "        ###Note : model prediction on default size images without resizing the image as model will resize internally and give the outputs matching with original size images\n",
    "        outputs = model(image)\n",
    "    \n",
    "    ###load all detection to CPU for further operations\n",
    "    outputs = [{k: v.to('cpu') for k, v in t.items()} for t in outputs]\n",
    "    \n",
    "    ###draw boxes, classes with score on images if above threshold and if boxes available\n",
    "    if len(outputs[0]['boxes']) != 0:\n",
    "        ###converting tensors to numpy\n",
    "        boxes = outputs[0]['boxes'].data.numpy()\n",
    "        scores = outputs[0]['scores'].data.numpy()\n",
    "        ###getting the max score above threshold from multiple boxes as only 1 box is required, works for custom backbone where single prediction can't be defined unlike pretrained backbone(given as parameter)\n",
    "        max_score = scores.max(where=(scores >= detection_threshold),initial=0)\n",
    "        max_score_index = np.where(scores==max_score)\n",
    "        ###filter out boxes according to `detection_threshold`, Note: similar to NMS but in FRCNN, NMS is internal\n",
    "        #boxes = boxes[scores >= detection_threshold].astype(np.int32)   ###works for frcnn with pretrained backbones having only 1 detection(given as parameter)\n",
    "        boxes = boxes[max_score_index].astype(np.int32) if(max_score >= detection_threshold) else []   ###if no single box matching threshold then empty\n",
    "        draw_boxes = boxes.copy()\n",
    "        ###get all the predicited class names\n",
    "        pred_classes = [CLASSES[i] for i in outputs[0]['labels'].cpu().numpy()]\n",
    "        \n",
    "        ###draw the bounding boxes and write the class name with score on top of it\n",
    "        for j, box in enumerate(draw_boxes):\n",
    "            \n",
    "            cv2.rectangle(orig_image,\n",
    "                        (int(box[0]), int(box[1])),\n",
    "                        (int(box[2]), int(box[3])),\n",
    "                        (0, 255, 0), 2)\n",
    "            cv2.putText(orig_image, pred_classes[j], \n",
    "                        (int(box[0]), int(box[1]-5)),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), \n",
    "                        2, lineType=cv2.LINE_AA)\n",
    "            cv2.putText(orig_image, str(round(scores[j],2)), \n",
    "                        (int(box[0]), int(box[1]-20)),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), \n",
    "                        2, lineType=cv2.LINE_AA)\n",
    "        \n",
    "    ###show the output frame\n",
    "    cv2.imshow(\"Outout_Frame\", orig_image)\n",
    "    vid_strm_save.write(orig_image)\n",
    "    \n",
    "    ###if the 'q' key was pressed, break from the loop\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "    ###update the FPS counter\n",
    "    fps.update()\n",
    "\n",
    "###stop the timer and display FPS information\n",
    "fps.stop()\n",
    "\n",
    "print(\"Elapsed Time: {:.2f}\".format(fps.elapsed()))\n",
    "print(\"Approx. FPS: {:.2f}\".format(fps.fps()))\n",
    "\n",
    "###cleanup of all objects created\n",
    "vid_strm.release()\n",
    "vid_strm_save.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f6081f-c560-4ac6-9e8b-f2439e0d8fc9",
   "metadata": {},
   "source": [
    "ONNX Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951d268c-af76-4c54-bd48-0a772407bf5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###converting the .pt to onnx\n",
    "###Note: conversion should happen without cuda device on cpu as it throws error \"https://github.com/pytorch/pytorch/issues/72175\"\n",
    "\n",
    "###set the computation device\n",
    "#device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "###load the model and the trained weights\n",
    "model = create_model_frcnn_custombackbone(num_classes=3)#.to(device)\n",
    "###to load the model correctly, get the model's state dict and then use/run it\n",
    "model.load_state_dict(torch.load(\n",
    "    OUT_DIR+\"\\\\FRCNN_CustomBackbone_Outputs\\\\Model_Graphs_FRCNN_CustomBackbone\\\\\"+MODEL_NAME+\"_ckpt_48.pt\")['state_dict'])   ###saved as pt and not pth because collides with python path library\n",
    "###get the model into evaluation mode\n",
    "model.eval()\n",
    "\n",
    "###input tensor to the model\n",
    "onnx_input = torch.randn(1, 3, RESIZE_TO, RESIZE_TO, requires_grad=True)#.to(device)\n",
    "#model_onnx_input = model(onnx_input)\n",
    "###path to save onnx model\n",
    "onnx_path = OUT_DIR + \"\\\\FRCNN_CustomBackbone_Outputs\\\\ONNX_FRCNN_CustomBackbone\\\\\" + MODEL_NAME + \"_ckpt48_onnx_varsize.onnx\"\n",
    "\n",
    "###export the model to onnx for variable length axes\n",
    "torch.onnx.export(model,                     ###model being run\n",
    "                  onnx_input,                ###model input (or a tuple for multiple inputs)\n",
    "                  onnx_path,   ###where to save the model (can be a file or file-like object)\n",
    "                  export_params=True,        ###store the trained parameter weights inside the model file\n",
    "                  opset_version=11,          ###the ONNX version to export the model to(keep 11 as 10 throws error with onnx==1.11)\n",
    "                  verbose=True,              ###prints the model conversion/debug process\n",
    "                  input_names = ['input'],   ###the model's input names\n",
    "                  output_names = ['output'], ###the model's output names\n",
    "                  do_constant_folding=True,  ###whether to execute constant folding for optimization\n",
    "                  dynamic_axes={\"input\": {2: \"H\", 3: \"W\"},\n",
    "                               \"output\": {2: \"H\", 3: \"W\"},\n",
    "                                })           ###make the onnx to accept different sizes of images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0291864-535d-45df-bd85-696407b7e90b",
   "metadata": {},
   "source": [
    "ONNX Model Testing - Test Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673335d2-e1a6-4d93-89bd-d51d8057286c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_time = time.time()\n",
    "###Load the ONNX model\n",
    "onnx_model = onnx.load(OUT_DIR + \"\\\\FRCNN_CustomBackbone_Outputs\\\\ONNX_FRCNN_CustomBackbone\\\\\"+MODEL_NAME+\"_ckpt48_onnx_varsize.onnx\")\n",
    "###Create an ONNX runtime session with both CUDA and CPU to run the onnx using cuda if available\n",
    "ort_session = onnxruntime.InferenceSession(OUT_DIR + \"\\\\FRCNN_CustomBackbone_Outputs\\\\ONNX_FRCNN_CustomBackbone\\\\\"+MODEL_NAME+\"_ckpt48_onnx_varsize.onnx\", \n",
    "                                           providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\n",
    "\n",
    "###csv name to store metrics\n",
    "out_csv = MODEL_NAME + \"_ONNX_MetricsCSV_ckpt48.csv\"\n",
    "###directory where all the images and its annotation json are present\n",
    "DIR_TEST = \"G:\\\\Projects\\\\ADAS\\\\DDW\\\\Data\\\\Usable_Data\\\\test\"\n",
    "test_images = glob.glob(f\"{DIR_TEST}/*.jpg\")\n",
    "print(f\"Test instances: {len(test_images)}\")\n",
    "\n",
    "###read the coco json to get all the ids, classes/annotations, boxes to make as groundtruth for metrics calculation : TP, FP, FN, Precision, Recall, F1 etc.\n",
    "coco = COCO(DIR_TEST + \"\\\\test_3_annotations.coco.json\")\n",
    "\n",
    "ids_l = list(sorted(coco.imgs.keys()))\n",
    "imgname_l = [coco.loadImgs([i])[0]['file_name'] for i in ids_l]   ###[0] used because for 1 id only 1 image and 1 class annotation, if multiole then [0]...[n]\n",
    "ann_ids_l = [coco.getAnnIds(imgIds=ids_l[i])[0] for i in ids_l]\n",
    "ann_cls_l = [coco.loadAnns(i)[0]['category_id'] for i in ann_ids_l]\n",
    "bbox_l = [coco.loadAnns(i)[0]['bbox'] for i in ann_ids_l]\n",
    "\n",
    "###classes: 0 index is reserved for background\n",
    "CLASSES = ['background', 'awake', 'drowsy']\n",
    "###define the detection threshold, any detection having score below this will be discarded\n",
    "detection_threshold = 0.75\n",
    "###dictionary to store count, TP, FP, FN, IoU, Precision, Recall, F1 of each class and avg of all\n",
    "metrics_dict = {}\n",
    "for cls in CLASSES:\n",
    "    if cls != \"background\":\n",
    "        metrics_dict[cls+\"_Count\"] = 0\n",
    "        metrics_dict[cls+\"_TP\"] = 0\n",
    "        metrics_dict[cls+\"_FP\"] = 0\n",
    "        metrics_dict[cls+\"_FN\"] = 0\n",
    "        metrics_dict[cls+\"_IoU\"] = 0\n",
    "        metrics_dict[cls+\"_Precision\"] = 0\n",
    "        metrics_dict[cls+\"_Recall\"] = 0\n",
    "        metrics_dict[cls+\"_F1\"] = 0\n",
    "metrics_dict[\"Avg_IoU\"] = 0\n",
    "metrics_dict[\"Avg_Precision\"] = 0\n",
    "metrics_dict[\"Avg_Recall\"] = 0\n",
    "metrics_dict[\"Avg_F1\"] = 0\n",
    "\n",
    "###loop through all test images\n",
    "for i in range(len(test_images)):\n",
    "    each_img_time = time.time()\n",
    "    ###get the image file name for saving output later on\n",
    "    image_name = test_images[i].split('\\\\')[-1]\n",
    "    print(\"Image Name = \", image_name)\n",
    "    ###read the image using cv2\n",
    "    image = cv2.imread(test_images[i])\n",
    "    ###get image size for further groundtruth box resizing before image resize\n",
    "    hght, wdth, *_ = image.shape\n",
    "    ###resize the image if onnx expects fix size to which it was converted to make the detections faster\n",
    "    #image = cv2.resize(image, (RESIZE_TO, RESIZE_TO))\n",
    "    ###take a copy of image to draw boxes and classes name with score\n",
    "    orig_image = image.copy()\n",
    "    ###BGR to RGB\n",
    "    image = cv2.cvtColor(orig_image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "    ###bring color channels to front followed by width and height\n",
    "    image = np.transpose(image, (2, 0, 1)).astype(np.float32)   ###np.float isn't supported in numpy==1.22 so np.float64 or float32 which is equivalant\n",
    "    ###convert to tensor\n",
    "    image = torch.tensor(image, dtype=torch.float)#.cuda()   ###numpy doesn't support cuda at line 84 while converting image to numpy\n",
    "    ###make the pixel range between 0 and 1\n",
    "    image /= 255.0\n",
    "    ###add batch dimension\n",
    "    image = torch.unsqueeze(image, 0)\n",
    "\n",
    "    ###get the index of the matching image name from coco ids list to get other info like class, box for current image(which acts as groundtruth)\n",
    "    cur_id_index = imgname_l.index(image_name)\n",
    "    cur_ann_cls = ann_cls_l[cur_id_index]\n",
    "    cur_box = bbox_l[cur_id_index]\n",
    "    ###to convert width and height of box into xmax/x2, ymax/y2 and not to resize based on model trained image size i.e., 416 because model will give outputs matching with original size images\n",
    "    cur_box[2] = cur_box[0] + cur_box[2]\n",
    "    cur_box[3] = cur_box[1] + cur_box[3]\n",
    "    '''cur_box[0] = (cur_box[0]/wdth)*416\n",
    "    cur_box[2] = (cur_box[0] + cur_box[2]/wdth)*416\n",
    "    cur_box[1] = (cur_box[1]/hght)*416\n",
    "    cur_box[3] = (cur_box[1] + cur_box[3]/hght)*416'''\n",
    "\n",
    "    ###Run the ONNX model on input image\n",
    "    inputs = {\"input\": image.numpy()}\n",
    "    outputs = ort_session.run(None, inputs)\n",
    "    \n",
    "    ###draw boxes, classes with score on images if above threshold and if boxes available\n",
    "    if len(outputs[0]) != 0:\n",
    "        boxes = outputs[0]#.data.numpy()\n",
    "        scores = outputs[2]#.data.numpy()\n",
    "        ###getting the max score above threshold from multiple boxes as only 1 box is required, works for custom backbone where single prediction can't be defined unlike pretrained backbone(given as parameter)\n",
    "        max_score = scores.max(where=(scores >= detection_threshold),initial=0)\n",
    "        max_score_index = np.where(scores==max_score)\n",
    "        ###filter out boxes according to `detection_threshold`, Note: similar to NMS but in FRCNN, NMS is internal\n",
    "        #boxes = boxes[scores >= detection_threshold].astype(np.int32)   ###works for frcnn with pretrained backbones having only 1 detection(given as parameter)\n",
    "        boxes = boxes[max_score_index].astype(np.int32) if(max_score >= detection_threshold) else []   ###if no single box matching threshold then empty\n",
    "        draw_boxes = boxes.copy()\n",
    "        ###get all the predicited class names\n",
    "        pred_classes = [CLASSES[i] for i in outputs[1]]\n",
    "        \n",
    "        ###draw the bounding boxes and write the class name with score on top of it\n",
    "        for j, box in enumerate(draw_boxes):\n",
    "            \n",
    "            ###get the counts of TP, FP for each class in dict\n",
    "            if(CLASSES[cur_ann_cls] == pred_classes[j]):   ###TP case(score is already > threshold)\n",
    "                metrics_dict[pred_classes[j]+'_TP'] = metrics_dict.get(pred_classes[j]+'_TP')+1\n",
    "            else:   ###FP case(score is already > threshold)\n",
    "                metrics_dict[pred_classes[j]+'_FP'] = metrics_dict.get(pred_classes[j]+'_FP')+1\n",
    "            ###get the avg IoU for each class in dict\n",
    "            cur_IoU = IoU_Calc(cur_box, box)\n",
    "            metrics_dict[pred_classes[j]+'_IoU'] = round((metrics_dict.get(pred_classes[j]+'_IoU') + cur_IoU),2)\n",
    "\n",
    "            ###draw groundtruth on images\n",
    "            cv2.rectangle(orig_image,\n",
    "                        (int(cur_box[0]), int(cur_box[1])),\n",
    "                        (int(cur_box[2]), int(cur_box[3])),\n",
    "                        (0, 0, 255), 2)\n",
    "            cv2.putText(orig_image, CLASSES[cur_ann_cls], \n",
    "                        (int(cur_box[0]), int(cur_box[3]+10)),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), \n",
    "                        2, lineType=cv2.LINE_AA)\n",
    "            \n",
    "            ###draw model predictions on images\n",
    "            cv2.rectangle(orig_image,\n",
    "                        (int(box[0]), int(box[1])),\n",
    "                        (int(box[2]), int(box[3])),\n",
    "                        (0, 255, 0), 2)\n",
    "            cv2.putText(orig_image, pred_classes[j], \n",
    "                        (int(box[0]), int(box[1]-5)),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), \n",
    "                        2, lineType=cv2.LINE_AA)\n",
    "            cv2.putText(orig_image, str(round(scores[j],2)), \n",
    "                        (int(box[0]), int(box[1]-20)),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), \n",
    "                        2, lineType=cv2.LINE_AA)\n",
    "        \n",
    "        #cv2.imshow('Prediction', orig_image)\n",
    "        #cv2.waitKey(1)\n",
    "\n",
    "        ###get the counts of FN, when model doesn't detects a groundtruth due to < threshold\n",
    "        metrics_dict[CLASSES[cur_ann_cls]+'_FN'] = metrics_dict.get(CLASSES[cur_ann_cls]+'_FN')+1 if(len(draw_boxes) == 0) else metrics_dict.get(CLASSES[cur_ann_cls]+'_FN')      \n",
    "        \n",
    "        ###save the predicted images\n",
    "        path = OUT_DIR + \"\\\\FRCNN_CustomBackbone_Outputs\\\\Test_Predictions_ONNX_FRCNN_CustomBackbone\\\\\" + image_name\n",
    "        cv2.imwrite(path, orig_image)\n",
    "        \n",
    "    ###get the counts of FN, when model doesn't detects a groundtruth at all\n",
    "    metrics_dict[CLASSES[cur_ann_cls]+'_FN'] = metrics_dict.get(CLASSES[cur_ann_cls]+'_FN')+1 if(len(outputs[0]) == 0) else metrics_dict.get(CLASSES[cur_ann_cls]+'_FN')\n",
    "    ###get the counts of each groundtruth class in dict\n",
    "    metrics_dict[CLASSES[cur_ann_cls]+'_Count'] = metrics_dict.get(CLASSES[cur_ann_cls]+'_Count')+1  \n",
    "\n",
    "    print(f\"Image {i+1} Done with Time : \", str(time.time() - each_img_time))\n",
    "    print('-'*50)\n",
    "\n",
    "'''\n",
    "###1 class's FP is another class's FN in binary classification if background class is neglected, so swap the FPs for FNs only when model predicts all above threshold\n",
    "metrics_dict[str(CLASSES[1])+\"_FN\"] = metrics_dict.get(CLASSES[2]+'_FP')\n",
    "metrics_dict[str(CLASSES[2])+\"_FN\"] = metrics_dict.get(CLASSES[1]+'_FP')\n",
    "'''\n",
    "###to calculate the avg IoU\n",
    "metrics_dict = IoU_Precision_Recall_F1(metrics_dict, CLASSES)\n",
    "print(\"\\nDifferent Metrics = \", metrics_dict)\n",
    "\n",
    "###save the metrics in csv using pandas\n",
    "df = pd.DataFrame([metrics_dict]) \n",
    "df.to_csv (OUT_DIR + \"\\\\FRCNN_CustomBackbone_Outputs\\\\Test_Predictions_ONNX_FRCNN_CustomBackbone\\\\\" + out_csv, index=False, header=True)\n",
    "\n",
    "#cv2.destroyAllWindows()\n",
    "print('\\nTEST PREDICTIONS COMPLETE WITH TIME : ',str(time.time() - all_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de45263-7524-4d17-980d-8a3f3b61b549",
   "metadata": {},
   "source": [
    "ONNX Model Testing - Video/Webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0f773e-dfcd-4d24-92ab-1f9c30da7c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Load the ONNX model\n",
    "onnx_model = onnx.load(OUT_DIR + \"\\\\ONNX\\\\\"+MODEL_NAME+\"_ckpt48_onnx.onnx\")\n",
    "###Create an ONNX runtime session with both CUDA and CPU to run the onnx using cuda if available\n",
    "ort_session = onnxruntime.InferenceSession(OUT_DIR + \"\\\\ONNX\\\\\"+MODEL_NAME+\"_ckpt48_onnx_varsize.onnx\", \n",
    "                                           providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\n",
    "\n",
    "###classes: 0 index is reserved for background\n",
    "CLASSES = ['background', 'awake', 'drowsy']\n",
    "###define the detection threshold, any detection having score below this will be discarded\n",
    "detection_threshold = 0.75\n",
    "\n",
    "###initialize the video stream\n",
    "vid_strm = cv2.VideoCapture(\"https://192.168.0.104:8080/video\")   ###from android app webcam else give src=0 for local cam\n",
    "###save the video stream used for predictions\n",
    "vid_strm_save = cv2.VideoWriter(OUT_DIR + \"\\\\Video_Output_ONNX\\\\\" + MODEL_NAME + \"ckpt48_onnx_webcam.mp4\",cv2.VideoWriter_fourcc(*'MP4V'),\n",
    "                         10,(1920,1080))\n",
    "\n",
    "###initialize the FPS counter\n",
    "fps = FPS().start()\n",
    "\n",
    "#while True:\n",
    "while(vid_strm.isOpened()):\n",
    "    ###read/get the frame/image from video stream/webcam\n",
    "    val, image = vid_strm.read()\n",
    "    #image = cv2.resize(image, (RESIZE_TO, RESIZE_TO))   ###if resized, model will run faster and fps increases\n",
    "    ###if no frame then exit the process\n",
    "    if image is None:\n",
    "        break\n",
    "    ###take a copy of image to draw boxes and classes name with score\n",
    "    orig_image = image.copy()\n",
    "    ###BGR to RGB\n",
    "    image = cv2.cvtColor(orig_image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "    ###bring color channels to front followed by width and height\n",
    "    image = np.transpose(image, (2, 0, 1)).astype(np.float32)   ###np.float isn't supported in numpy==1.22 so np.float64 or float32 which is equivalant\n",
    "    ###convert to tensor\n",
    "    image = torch.tensor(image, dtype=torch.float)#.cuda()   ###numpy doesn't support cuda at line 84 while converting image to numpy\n",
    "    ###make the pixel range between 0 and 1\n",
    "    image /= 255.0\n",
    "    ###add batch dimension\n",
    "    image = torch.unsqueeze(image, 0)\n",
    "\n",
    "    ###Run the ONNX model on input image\n",
    "    inputs = {\"input\": image.numpy()}\n",
    "    outputs = ort_session.run(None, inputs)\n",
    "    \n",
    "    ###draw boxes, classes with score on images if above threshold and if boxes available\n",
    "    if len(outputs[0]) != 0:\n",
    "        boxes = outputs[0]#.data.numpy()\n",
    "        scores = outputs[2]#.data.numpy()\n",
    "        ###getting the max score above threshold from multiple boxes as only 1 box is required, works for custom backbone where single prediction can't be defined unlike pretrained backbone(given as parameter)\n",
    "        max_score = scores.max(where=(scores >= detection_threshold),initial=0)\n",
    "        max_score_index = np.where(scores==max_score)\n",
    "        ###filter out boxes according to `detection_threshold`, Note: similar to NMS but in FRCNN, NMS is internal\n",
    "        #boxes = boxes[scores >= detection_threshold].astype(np.int32)   ###works for frcnn with pretrained backbones having only 1 detection(given as parameter)\n",
    "        boxes = boxes[max_score_index].astype(np.int32) if(max_score >= detection_threshold) else []   ###if no single box matching threshold then empty\n",
    "        draw_boxes = boxes.copy()\n",
    "        ###get all the predicited class names\n",
    "        pred_classes = [CLASSES[i] for i in outputs[1]]\n",
    "        \n",
    "        ###draw the bounding boxes and write the class name with score on top of it\n",
    "        for j, box in enumerate(draw_boxes):\n",
    "            \n",
    "            cv2.rectangle(orig_image,\n",
    "                        (int(box[0]), int(box[1])),\n",
    "                        (int(box[2]), int(box[3])),\n",
    "                        (0, 255, 0), 2)\n",
    "            cv2.putText(orig_image, pred_classes[j], \n",
    "                        (int(box[0]), int(box[1]-5)),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), \n",
    "                        2, lineType=cv2.LINE_AA)\n",
    "            cv2.putText(orig_image, str(round(scores[j],2)), \n",
    "                        (int(box[0]), int(box[1]-20)),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), \n",
    "                        2, lineType=cv2.LINE_AA)\n",
    "        \n",
    "    ###show the output frame\n",
    "    cv2.imshow(\"Outout_Frame\", orig_image)\n",
    "    vid_strm_save.write(orig_image)\n",
    "    \n",
    "    ###if the 'q' key was pressed, break from the loop\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "    ###update the FPS counter\n",
    "    fps.update()\n",
    "\n",
    "###stop the timer and display FPS information\n",
    "fps.stop()\n",
    "\n",
    "print(\"Elapsed Time: {:.2f}\".format(fps.elapsed()))\n",
    "print(\"Approx. FPS: {:.2f}\".format(fps.fps()))\n",
    "\n",
    "###cleanup of all objects created\n",
    "vid_strm.release()\n",
    "vid_strm_save.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf0e25a-a301-4503-9617-471a7290b3db",
   "metadata": {},
   "source": [
    "OpenVINO Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f7e3cb-0d1b-47f7-a67c-37ab93f2a847",
   "metadata": {},
   "outputs": [],
   "source": [
    "###conversion of onnx to openvino=2023.1(latest)\n",
    "\n",
    "###onnx saved path\n",
    "onnx_path = OUT_DIR + \"\\\\FRCNN_CustomBackbone_Outputs\\\\ONNX_FRCNN_CustomBackbone\\\\\" + MODEL_NAME + \"_ckpt48_onnx_varsize.onnx\"\n",
    "###path to save openvino model(consists of .xml & .bin)\n",
    "ir_path = OUT_DIR + \"\\\\FRCNN_CustomBackbone_Outputs\\\\OpenVINO_FRCNN_CustomBackbone\\\\\" + MODEL_NAME + \"_ckpt48_openvino_varsize.xml\"\n",
    "\n",
    "###conversion and saving wiht compression to fp16(to improve performance but accuracy will be affected a bit) instead of fp32(onnx model is of fp32)\n",
    "ov_model = ov.convert_model(onnx_path)   ###if not to convert to fp16 and keep default then add , compress_to_fp16=False given in \"https://docs.openvino.ai/2023.1/openvino_docs_MO_DG_FP16_Compression.html\"\n",
    "#ov_model = mo.convert_model(onnx_path, input_shape=[1,3,416,416], compress_to_fp16=True)   ###only mo supports input_shape and other options\n",
    "#ov_model = mo.convert_model(help=True)   ###to get different options available\n",
    "ov.save_model(ov_model, ir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93abe92-ee95-4a88-a80e-bb97c2337ba0",
   "metadata": {},
   "source": [
    "OpenVINO Model Testing - Test Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8536494-669d-4631-8e70-4edd7526d8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_time = time.time()\n",
    "###intialize the OpenVINO Core package\n",
    "core = ov.Core()\n",
    "###read the OpenVINO model(.xml)\n",
    "model = core.read_model(model=OUT_DIR + \"\\\\FRCNN_CustomBackbone_Outputs\\\\OpenVINO_FRCNN_CustomBackbone\\\\\" + MODEL_NAME + \"_ckpt48_openvino_varsize.xml\")\n",
    "###compile the OpenVINO model\n",
    "compiled_model = core.compile_model(model=model, device_name=\"CPU\")   ###check for available devices in system using \"from openvino.runtime import Core; print(Core().available_devices)\" and GPU.1 means NVIDIA and GPU.0 means Intel\n",
    "                                                                      ###Note: if GPU/GPU.0 used then throws unknown error as \"[GPU] get_tensor() is called for dynamic shape without upper bound\" and GPU.1 is NVIDIA so not supported by Intel's OpenVINO\n",
    "#input_layer = compiled_model.input(0) #(0..2)   ###check the input layer\n",
    "#output_layer = compiled_model.output(0) #(0..2)   ###get the outputs from compiled openvino model\n",
    "\n",
    "###csv name to store metrics\n",
    "out_csv = MODEL_NAME + \"_OpenVINO_MetricsCSV_ckpt48.csv\"\n",
    "###directory where all the images and its annotation json are present\n",
    "DIR_TEST = \"G:\\\\Projects\\\\ADAS\\\\DDW\\\\Data\\\\Usable_Data\\\\test\"\n",
    "test_images = glob.glob(f\"{DIR_TEST}/*.jpg\")\n",
    "print(f\"Test instances: {len(test_images)}\")\n",
    "\n",
    "###read the coco json to get all the ids, classes/annotations, boxes to make as groundtruth for metrics calculation : TP, FP, FN, Precision, Recall, F1 etc.\n",
    "coco = COCO(DIR_TEST + \"\\\\test_3_annotations.coco.json\")\n",
    "\n",
    "ids_l = list(sorted(coco.imgs.keys()))\n",
    "imgname_l = [coco.loadImgs([i])[0]['file_name'] for i in ids_l]   ###[0] used because for 1 id only 1 image and 1 class annotation, if multiole then [0]...[n]\n",
    "ann_ids_l = [coco.getAnnIds(imgIds=ids_l[i])[0] for i in ids_l]\n",
    "ann_cls_l = [coco.loadAnns(i)[0]['category_id'] for i in ann_ids_l]\n",
    "bbox_l = [coco.loadAnns(i)[0]['bbox'] for i in ann_ids_l]\n",
    "\n",
    "###classes: 0 index is reserved for background\n",
    "CLASSES = ['background', 'awake', 'drowsy']\n",
    "###define the detection threshold, any detection having score below this will be discarded\n",
    "detection_threshold = 0.75\n",
    "###dictionary to store count, TP, FP, FN, IoU, Precision, Recall, F1 of each class and avg of all\n",
    "metrics_dict = {}\n",
    "for cls in CLASSES:\n",
    "    if cls != \"background\":\n",
    "        metrics_dict[cls+\"_Count\"] = 0\n",
    "        metrics_dict[cls+\"_TP\"] = 0\n",
    "        metrics_dict[cls+\"_FP\"] = 0\n",
    "        metrics_dict[cls+\"_FN\"] = 0\n",
    "        metrics_dict[cls+\"_IoU\"] = 0\n",
    "        metrics_dict[cls+\"_Precision\"] = 0\n",
    "        metrics_dict[cls+\"_Recall\"] = 0\n",
    "        metrics_dict[cls+\"_F1\"] = 0\n",
    "metrics_dict[\"Avg_IoU\"] = 0\n",
    "metrics_dict[\"Avg_Precision\"] = 0\n",
    "metrics_dict[\"Avg_Recall\"] = 0\n",
    "metrics_dict[\"Avg_F1\"] = 0\n",
    "\n",
    "###loop through all test images\n",
    "for i in range(len(test_images)):\n",
    "    each_img_time = time.time()\n",
    "    ###get the image file name for saving output later on\n",
    "    image_name = test_images[i].split('\\\\')[-1]\n",
    "    print(\"Image Name = \", image_name)\n",
    "    ###read the image using cv2\n",
    "    image = cv2.imread(test_images[i])\n",
    "    ###get image size for further groundtruth box resizing before image resize\n",
    "    hght, wdth, *_ = image.shape\n",
    "    ###resize the image if openvino expects fix size to which it was converted to make the detections faster\n",
    "    #image = cv2.resize(image, (RESIZE_TO, RESIZE_TO))\n",
    "    ###take a copy of image to draw boxes and classes name with score\n",
    "    orig_image = image.copy()\n",
    "    ###BGR to RGB\n",
    "    image = cv2.cvtColor(orig_image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "    ###bring color channels to front followed by width and height\n",
    "    image = np.transpose(image, (2, 0, 1)).astype(np.float32)   ###np.float isn't supported in numpy==1.22 so np.float64 or float32 which is equivalant\n",
    "    ###convert to tensor\n",
    "    image = torch.tensor(image, dtype=torch.float)#.cuda()   ###numpy doesn't support cuda at line 84 while converting image to numpy\n",
    "    ###make the pixel range between 0 and 1\n",
    "    image /= 255.0\n",
    "    ###add batch dimension\n",
    "    image = torch.unsqueeze(image, 0)\n",
    "\n",
    "    ###get the index of the matching image name from coco ids list to get other info like class, box for current image(which acts as groundtruth)\n",
    "    cur_id_index = imgname_l.index(image_name)\n",
    "    cur_ann_cls = ann_cls_l[cur_id_index]\n",
    "    cur_box = bbox_l[cur_id_index]\n",
    "    ###to convert width and height of box into xmax/x2, ymax/y2 and not to resize based on model trained image size i.e., 416 because model will give outputs matching with original size images\n",
    "    cur_box[2] = cur_box[0] + cur_box[2]\n",
    "    cur_box[3] = cur_box[1] + cur_box[3]\n",
    "    '''cur_box[0] = (cur_box[0]/wdth)*416\n",
    "    cur_box[2] = (cur_box[0] + cur_box[2]/wdth)*416\n",
    "    cur_box[1] = (cur_box[1]/hght)*416\n",
    "    cur_box[3] = (cur_box[1] + cur_box[3]/hght)*416'''\n",
    "\n",
    "    ###append the results of compiled OpenVINO model into outputs list\n",
    "    outputs = []\n",
    "    for out_ov in range(3):\n",
    "        outputs.append(compiled_model(image)[compiled_model.output(out_ov)])\n",
    "    #results = compiled_model(image)[output_layer]\n",
    "    \n",
    "    ###draw boxes, classes with score on images if above threshold and if boxes available\n",
    "    if len(outputs[0]) != 0:\n",
    "        boxes = outputs[0]#.data.numpy()\n",
    "        scores = outputs[2]#.data.numpy()\n",
    "        ###getting the max score above threshold from multiple boxes as only 1 box is required, works for custom backbone where single prediction can't be defined unlike pretrained backbone(given as parameter)\n",
    "        max_score = scores.max(where=(scores >= detection_threshold),initial=0)\n",
    "        max_score_index = np.where(scores==max_score)\n",
    "        ###filter out boxes according to `detection_threshold`, Note: similar to NMS but in FRCNN, NMS is internal\n",
    "        #boxes = boxes[scores >= detection_threshold].astype(np.int32)   ###works for frcnn with pretrained backbones having only 1 detection(given as parameter)\n",
    "        boxes = boxes[max_score_index].astype(np.int32) if(max_score >= detection_threshold) else []   ###if no single box matching threshold then empty\n",
    "        draw_boxes = boxes.copy()\n",
    "        ###get all the predicited class names\n",
    "        pred_classes = [CLASSES[i] for i in outputs[1]]\n",
    "        \n",
    "        ###draw the bounding boxes and write the class name with score on top of it\n",
    "        for j, box in enumerate(draw_boxes):\n",
    "            \n",
    "            ###get the counts of TP, FP for each class in dict\n",
    "            if(CLASSES[cur_ann_cls] == pred_classes[j]):   ###TP case(score is already > threshold)\n",
    "                metrics_dict[pred_classes[j]+'_TP'] = metrics_dict.get(pred_classes[j]+'_TP')+1\n",
    "            else:   ###FP case(score is already > threshold)\n",
    "                metrics_dict[pred_classes[j]+'_FP'] = metrics_dict.get(pred_classes[j]+'_FP')+1\n",
    "            ###get the avg IoU for each class in dict\n",
    "            cur_IoU = IoU_Calc(cur_box, box)\n",
    "            metrics_dict[pred_classes[j]+'_IoU'] = round((metrics_dict.get(pred_classes[j]+'_IoU') + cur_IoU),2)\n",
    "\n",
    "            ###draw groundtruth on images\n",
    "            cv2.rectangle(orig_image,\n",
    "                        (int(cur_box[0]), int(cur_box[1])),\n",
    "                        (int(cur_box[2]), int(cur_box[3])),\n",
    "                        (0, 0, 255), 2)\n",
    "            cv2.putText(orig_image, CLASSES[cur_ann_cls], \n",
    "                        (int(cur_box[0]), int(cur_box[3]+10)),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), \n",
    "                        2, lineType=cv2.LINE_AA)\n",
    "            \n",
    "            ###draw model predictions on images\n",
    "            cv2.rectangle(orig_image,\n",
    "                        (int(box[0]), int(box[1])),\n",
    "                        (int(box[2]), int(box[3])),\n",
    "                        (0, 255, 0), 2)\n",
    "            cv2.putText(orig_image, pred_classes[j], \n",
    "                        (int(box[0]), int(box[1]-5)),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), \n",
    "                        2, lineType=cv2.LINE_AA)\n",
    "            cv2.putText(orig_image, str(round(scores[j],2)), \n",
    "                        (int(box[0]), int(box[1]-20)),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), \n",
    "                        2, lineType=cv2.LINE_AA)\n",
    "        \n",
    "        #cv2.imshow('Prediction', orig_image)\n",
    "        #cv2.waitKey(1)\n",
    "\n",
    "        ###get the counts of FN, when model doesn't detects a groundtruth due to < threshold\n",
    "        metrics_dict[CLASSES[cur_ann_cls]+'_FN'] = metrics_dict.get(CLASSES[cur_ann_cls]+'_FN')+1 if(len(draw_boxes) == 0) else metrics_dict.get(CLASSES[cur_ann_cls]+'_FN')      \n",
    "        \n",
    "        ###save the predicted images\n",
    "        path = OUT_DIR + \"\\\\FRCNN_CustomBackbone_Outputs\\\\Test_Predictions_OpenVINO_FRCNN_CustomBackbone\\\\\" + image_name\n",
    "        cv2.imwrite(path, orig_image)\n",
    "        \n",
    "    ###get the counts of FN, when model doesn't detects a groundtruth at all\n",
    "    metrics_dict[CLASSES[cur_ann_cls]+'_FN'] = metrics_dict.get(CLASSES[cur_ann_cls]+'_FN')+1 if(len(outputs[0]) == 0) else metrics_dict.get(CLASSES[cur_ann_cls]+'_FN')\n",
    "    ###get the counts of each groundtruth class in dict\n",
    "    metrics_dict[CLASSES[cur_ann_cls]+'_Count'] = metrics_dict.get(CLASSES[cur_ann_cls]+'_Count')+1  \n",
    "\n",
    "    print(f\"Image {i+1} Done with Time : \", str(time.time() - each_img_time))\n",
    "    print('-'*50)\n",
    "\n",
    "'''\n",
    "###1 class's FP is another class's FN in binary classification if background class is neglected, so swap the FPs for FNs only when model predicts all above threshold\n",
    "metrics_dict[str(CLASSES[1])+\"_FN\"] = metrics_dict.get(CLASSES[2]+'_FP')\n",
    "metrics_dict[str(CLASSES[2])+\"_FN\"] = metrics_dict.get(CLASSES[1]+'_FP')\n",
    "'''\n",
    "###to calculate the avg IoU\n",
    "metrics_dict = IoU_Precision_Recall_F1(metrics_dict, CLASSES)\n",
    "print(\"\\nDifferent Metrics = \", metrics_dict)\n",
    "\n",
    "###save the metrics in csv using pandas\n",
    "df = pd.DataFrame([metrics_dict]) \n",
    "df.to_csv (OUT_DIR + \"\\\\FRCNN_CustomBackbone_Outputs\\\\Test_Predictions_OpenVINO_FRCNN_CustomBackbone\\\\\" + out_csv, index=False, header=True)\n",
    "\n",
    "#cv2.destroyAllWindows()\n",
    "print('\\nTEST PREDICTIONS COMPLETE WITH TIME : ',str(time.time() - all_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d26a64-5d9a-4770-a692-6722b7fbbf6f",
   "metadata": {},
   "source": [
    "OpenVINO Model Testing - Video/Webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386a2326-e6c6-43c8-bccc-c6a88bea8844",
   "metadata": {},
   "outputs": [],
   "source": [
    "###intialize the OpenVINO Core package\n",
    "core = ov.Core()\n",
    "###read the OpenVINO model(.xml)\n",
    "model = core.read_model(model=OUT_DIR + \"\\\\OpenVINO\\\\\" + MODEL_NAME + \"_ckpt48_openvino_varsize.xml\")\n",
    "###compile the OpenVINO model\n",
    "compiled_model = core.compile_model(model=model, device_name=\"CPU\")   ###check for available devices in system using \"from openvino.runtime import Core; print(Core().available_devices)\" and GPU.1 means NVIDIA and GPU.0 means Intel\n",
    "                                                                      ###Note: if GPU or GPU.0/GPU used then throws unknown error and GPU.1 is NVIDIA so not supported by Intel's OpenVINO\n",
    "#input_layer = compiled_model.input(0) #(0..2)   ###check the input layer\n",
    "#output_layer = compiled_model.output(0) #(0..2)   ###get the outputs from compiled openvino model\n",
    "\n",
    "###classes: 0 index is reserved for background\n",
    "CLASSES = ['background', 'awake', 'drowsy']\n",
    "###define the detection threshold, any detection having score below this will be discarded\n",
    "detection_threshold = 0.75\n",
    "\n",
    "###initialize the video stream\n",
    "vid_strm = cv2.VideoCapture(\"https://192.168.0.104:8080/video\")   ###from android app webcam else give src=0 for local cam\n",
    "###save the video stream used for predictions\n",
    "vid_strm_save = cv2.VideoWriter(OUT_DIR + \"\\\\Video_Output_OpenVINO\\\\\" + MODEL_NAME + \"ckpt48_openvino_webcam.mp4\",cv2.VideoWriter_fourcc(*'MP4V'),\n",
    "                         10,(1920,1080))\n",
    "\n",
    "###initialize the FPS counter\n",
    "fps = FPS().start()\n",
    "\n",
    "#while True:\n",
    "while(vid_strm.isOpened()):\n",
    "    ###read/get the frame/image from video stream/webcam\n",
    "    val, image = vid_strm.read()\n",
    "    #image = cv2.resize(image, (RESIZE_TO, RESIZE_TO))   ###if resized, model will run faster and fps increases\n",
    "    ###if no frame then exit the process\n",
    "    if image is None:\n",
    "        break\n",
    "    ###take a copy of image to draw boxes and classes name with score\n",
    "    orig_image = image.copy()\n",
    "    ###BGR to RGB\n",
    "    image = cv2.cvtColor(orig_image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "    ###bring color channels to front followed by width and height\n",
    "    image = np.transpose(image, (2, 0, 1)).astype(np.float32)   ###np.float isn't supported in numpy==1.22 so np.float64 or float32 which is equivalant\n",
    "    ###convert to tensor\n",
    "    image = torch.tensor(image, dtype=torch.float)#.cuda()   ###numpy doesn't support cuda at line 84 while converting image to numpy\n",
    "    ###make the pixel range between 0 and 1\n",
    "    image /= 255.0\n",
    "    ###add batch dimension\n",
    "    image = torch.unsqueeze(image, 0)\n",
    "\n",
    "    ###append the results of compiled OpenVINO model into outputs list\n",
    "    outputs = []\n",
    "    for out_ov in range(3):\n",
    "        outputs.append(compiled_model(image)[compiled_model.output(out_ov)])\n",
    "    #results = compiled_model(image)[output_layer]\n",
    "    \n",
    "    ###draw boxes, classes with score on images if above threshold and if boxes available\n",
    "    if len(outputs[0]) != 0:\n",
    "        boxes = outputs[0]#.data.numpy()\n",
    "        scores = outputs[2]#.data.numpy()\n",
    "        ###getting the max score above threshold from multiple boxes as only 1 box is required, works for custom backbone where single prediction can't be defined unlike pretrained backbone(given as parameter)\n",
    "        max_score = scores.max(where=(scores >= detection_threshold),initial=0)\n",
    "        max_score_index = np.where(scores==max_score)\n",
    "        ###filter out boxes according to `detection_threshold`, Note: similar to NMS but in FRCNN, NMS is internal\n",
    "        #boxes = boxes[scores >= detection_threshold].astype(np.int32)   ###works for frcnn with pretrained backbones having only 1 detection(given as parameter)\n",
    "        boxes = boxes[max_score_index].astype(np.int32) if(max_score >= detection_threshold) else []   ###if no single box matching threshold then empty\n",
    "        draw_boxes = boxes.copy()\n",
    "        ###get all the predicited class names\n",
    "        pred_classes = [CLASSES[i] for i in outputs[1]]\n",
    "        \n",
    "        ###draw the bounding boxes and write the class name with score on top of it\n",
    "        for j, box in enumerate(draw_boxes):\n",
    "            \n",
    "            cv2.rectangle(orig_image,\n",
    "                        (int(box[0]), int(box[1])),\n",
    "                        (int(box[2]), int(box[3])),\n",
    "                        (0, 255, 0), 2)\n",
    "            cv2.putText(orig_image, pred_classes[j], \n",
    "                        (int(box[0]), int(box[1]-5)),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), \n",
    "                        2, lineType=cv2.LINE_AA)\n",
    "            cv2.putText(orig_image, str(round(scores[j],2)), \n",
    "                        (int(box[0]), int(box[1]-20)),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), \n",
    "                        2, lineType=cv2.LINE_AA)\n",
    "        \n",
    "    ###show the output frame\n",
    "    cv2.imshow(\"Outout_Frame\", orig_image)\n",
    "    vid_strm_save.write(orig_image)\n",
    "    \n",
    "    ###if the 'q' key was pressed, break from the loop\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "    ###update the FPS counter\n",
    "    fps.update()\n",
    "\n",
    "###stop the timer and display FPS information\n",
    "fps.stop()\n",
    "\n",
    "print(\"Elapsed Time: {:.2f}\".format(fps.elapsed()))\n",
    "print(\"Approx. FPS: {:.2f}\".format(fps.fps()))\n",
    "\n",
    "###cleanup of all objects created\n",
    "vid_strm.release()\n",
    "vid_strm_save.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b01ad4-7292-4cb7-865c-85342575cbd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
